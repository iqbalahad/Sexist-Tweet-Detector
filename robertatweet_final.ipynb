{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1947,
     "status": "ok",
     "timestamp": 1737221907184,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "VLE1Co934lWn",
    "outputId": "3c0e8888-b735-489b-84a4-1ed96538be93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRBZzDY-YZMq"
   },
   "source": [
    "# **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7333,
     "status": "ok",
     "timestamp": 1737221914517,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "5gaCXdq23OBF",
    "outputId": "a9d5d7b0-c271-4f65-cd47-3b2f494d0b21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /usr/local/lib/python3.11/dist-packages (0.5.14)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from num2words) (0.6.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install num2words\n",
    "!pip install nltk\n",
    "\n",
    "import re\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from num2words import num2words\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from transformers import AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cm7vKDK4YuOs"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1737221914517,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "Gl5ErGPSYqkc"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('/content/drive/My Drive/Bachelorprojekt/val_data_no_stopwords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n0-bsYK4YxuW"
   },
   "source": [
    "# **Text-preprocessing**\n",
    "\n",
    "The `preprocessing` function takes a DataFrame as input, which must contain at least two columns: `labels` (for calculating scores) and `tweet` (containing raw text data). It processes the text by cleaning unwanted content, removing stopwords, and converting the cleaned data into a new column called `no stopwords`. Additionally, it calculates a `score` and a `class` column based on the `labels`. The function outputs the processed DataFrame, ready for further analysis or model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1737221914518,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "1IiLs_bP2di4"
   },
   "outputs": [],
   "source": [
    "def Data_pre_processing(df):\n",
    "\n",
    "    # Calculate the score\n",
    "    def calc_score(l):\n",
    "        counter = 0\n",
    "        for i in l:\n",
    "            if i == 'Y':\n",
    "                counter += 1\n",
    "        return counter\n",
    "\n",
    "    # Calculate the score and add the 'score' column\n",
    "    df[\"score\"] = df[\"labels\"].apply(calc_score)\n",
    "\n",
    "    # Classification based on the score\n",
    "    df[\"class\"] = df[\"score\"] >= 3\n",
    "    df[\"class\"] = df[\"class\"].astype(int)  # Ensure 'class' is stored as an integer\n",
    "\n",
    "    # Regex patterns for removing unwanted content\n",
    "    regex_link = r\"http(s?)://(([a-z]|[A-Z]|\\d)+\\.)+([a-z]|[A-Z]|\\d)+/?(([a-z]|[A-Z]|\\d)+/?)*\"\n",
    "    regex_email = r\"([a-z]|[A-Z]|\\d)+\\.([a-z]|[A-Z]|\\d)+\\@([a-z]|[A-Z]|\\d)+\\.([a-z]|[A-Z]|\\d)+\"\n",
    "    regex_tag = r\"@([a-z]|[A-Z]|_|\\d)+\"\n",
    "\n",
    "    # Function to remove unwanted content\n",
    "    def remove_unwanted(row):\n",
    "        cleaned_text = row[\"tweet\"].lower()\n",
    "        cleaned_text = re.sub(regex_link, \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(regex_email, \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(regex_tag, \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\",\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\":\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\.\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(ain\\'t)|(ain\\’t)\", \" is not\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(can\\'t)|(can\\’t)|(cant)\", \" can not\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(n\\'t)|(n\\’t)\", \" not\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'s)|(\\’s)\", \" is\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'d)|(\\’d)\", \" would\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'m)|(\\’m)\", \" am\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'re)|(\\’re)\", \" are\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'ve)|(\\’ve)\", \" have\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"(\\'ll)|(\\’ll)\", \" will\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"&amp;\", \" and \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"&gt;\", \" greater than \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"&lt;\", \" less than \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\su\\s\", \" you \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\'|\\||\\’|\\‘\", \"\", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\s\\s+\", \" \", cleaned_text)\n",
    "        cleaned_text = re.sub(r\"\\\"\", \"\", cleaned_text)\n",
    "\n",
    "        # Remove non-ASCII characters (like emojis)\n",
    "        cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
    "\n",
    "        # Convert numbers to words\n",
    "        cleaned_text = re.sub(r\"\\d\\d\\d\\d\", \"\", cleaned_text)  # Remove 4-digit numbers\n",
    "        cleaned_text = re.sub(r\"\\d+\", lambda x: num2words(x.group()), cleaned_text)\n",
    "\n",
    "        # Keep only allowed characters\n",
    "        cleaned_text = re.sub(r\"[^a-z|\\!|\\#|\\_|\\?|\\s|\\-]\", \"\", cleaned_text)\n",
    "\n",
    "        return cleaned_text\n",
    "\n",
    "    # Apply the function to clean the text\n",
    "    df[\"clean\"] = df.apply(remove_unwanted, axis=1)\n",
    "\n",
    "    # Initialize lemmatizer and stopwords\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    STOPWORDS = set(stopwords.words('english')) - {\"but\", \"nor\", \"not\", \"against\"}\n",
    "\n",
    "    # Function to remove stopwords\n",
    "    def remove_stopwords(text):\n",
    "        tweet = text.split()  # Split text into a list of words\n",
    "        clean = [lemmatizer.lemmatize(word) for word in tweet if word not in STOPWORDS]\n",
    "        return \" \".join(clean)\n",
    "\n",
    "    # Apply the function to remove stopwords\n",
    "    df[\"no stopwords\"] = df[\"clean\"].apply(remove_stopwords)\n",
    "\n",
    "    # Drop rows where 'no stopwords' is NaN\n",
    "    df = df.dropna(subset=['no stopwords'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OGnBlLmOZczk"
   },
   "source": [
    "# **Text Encoding**\n",
    "\n",
    "This method processes a DataFrame to prepare it for input into a trained model by converting preprocessed texts into numerical tokens. The method requires a DataFrame containing a column named 'no stopwords', which holds the cleaned and preprocessed text data, and a tokenizer compatible with the trained model. It extracts the text from the 'no stopwords' column and uses the tokenizer to generate tokenized representations, including Input IDs and Attention Masks. The output is a dictionary containing the tokenized data, ready for use in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 825,
     "status": "ok",
     "timestamp": 1737221915340,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "HOGK4M_E6twQ"
   },
   "outputs": [],
   "source": [
    "def Text_encoding(df, tokenizer = AutoTokenizer.from_pretrained('cardiffnlp/twitter-roberta-base-hate')):\n",
    "\n",
    "    # Extract the preprocessed texts\n",
    "    texts = df[\"no stopwords\"].tolist()\n",
    "\n",
    "    texts = [str(text) for text in texts]\n",
    "\n",
    "    # Tokenize the texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"  # Returns PyTorch tensors\n",
    "    )\n",
    "\n",
    "    return encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Helper function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Helper_function():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-YG8ubGmaSY2"
   },
   "source": [
    "# **Model Prediction**\n",
    "\n",
    "This method performs predictions using the trained model and calculates the accuracy and confusion matrix for the given set of tokenized data. It requires a trained model (e.g., a Transformer model), a dictionary containing tokenized input data (e.g., 'input_ids' and 'attention_mask'), and the true labels for evaluation. An optional classification threshold (default is 0.5 for binary classification) can be provided. The method returns a dictionary containing the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1737221915340,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "cGQr0xz4Dg5R"
   },
   "outputs": [],
   "source": [
    "def model_prediction(model, encodings, labels, threshold=0.5):\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure input data is moved to the correct device\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Perform predictions\n",
    "    with torch.no_grad():\n",
    "        input_ids = encodings[\"input_ids\"].to(device)\n",
    "        attention_mask = encodings[\"attention_mask\"].to(device)\n",
    "\n",
    "        # Get model outputs\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Calculate probabilities\n",
    "        probabilities = F.softmax(logits, dim=1)  # Softmax returns probabilities for each class\n",
    "        batch_predictions = (probabilities[:, 1] > threshold).int()  # Apply threshold (for binary classification)\n",
    "\n",
    "    # Calculate accuracy and confusion matrix\n",
    "    accuracy = accuracy_score(labels, batch_predictions.cpu().numpy())\n",
    "    conf_matrix = confusion_matrix(labels, batch_predictions.cpu().numpy())\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"confusion_matrix\": conf_matrix\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKWgibLCaw5G"
   },
   "source": [
    "# **Example Workflow: Model Evaluation**\n",
    "\n",
    "This example demonstrates the process of evaluating a trained model using preprocessed data. The workflow includes preprocessing the dataset, tokenizing the text for model input, loading the trained model, and performing predictions. Finally, it calculates and outputs the accuracy and confusion matrix, providing insights into the model's performance on the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16907,
     "status": "ok",
     "timestamp": 1737221932246,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "hKLk0zGfEDYl",
    "outputId": "88085a0c-2cdf-439a-a784-09d5add41a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9348914858096828\n",
      "Confusion Matrix:\n",
      "[[300  17]\n",
      " [ 22 260]]\n"
     ]
    }
   ],
   "source": [
    "df = Data_pre_processing(data)\n",
    "encodings = Text_encoding(df)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('/content/drive/My Drive/Bachelorprojekt/final_model1') # Specify the path to the trained model directory\n",
    "\n",
    "# Perform predictions\n",
    "results = model_prediction(model, encodings, data[\"class\"], threshold=0.5)\n",
    "\n",
    "# Output results\n",
    "print(f\"Accuracy: {results['accuracy']}\")\n",
    "print(f\"Confusion Matrix:\\n{results['confusion_matrix']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1737221932247,
     "user": {
      "displayName": "Fried Meyer",
      "userId": "15966779420388331557"
     },
     "user_tz": -60
    },
    "id": "804tgFWVI2Pu"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
